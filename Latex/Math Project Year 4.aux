\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand\tcolorbox@label[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{4}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Intro}{{1}{4}{Introduction}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}The Frequentist Vs The Bayesian}{4}{section.1.1}}
\newlabel{The F Vs B}{{1.1}{4}{The Frequentist Vs The Bayesian}{section.1.1}{}}
\citation{1}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Conditional Probability}{5}{section.1.2}}
\newlabel{Conditional Probability}{{1.2}{5}{Conditional Probability}{section.1.2}{}}
\newlabel{1.1}{{1.1}{5}{Conditional Probability}{equation.1.2.1}{}}
\citation{2}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}The Prior, Likelihood and Posterior}{6}{subsection.1.2.1}}
\newlabel{The Prior, Likelihood and Posterior}{{1.2.1}{6}{The Prior, Likelihood and Posterior}{subsection.1.2.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Tricky Denominators}{6}{section.1.3}}
\newlabel{Tricky Denominators}{{1.3}{6}{Tricky Denominators}{section.1.3}{}}
\newlabel{1.2}{{1.2}{6}{Tricky Denominators}{equation.1.3.2}{}}
\newlabel{1.3}{{1.3}{7}{Tricky Denominators}{equation.1.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Plot of the integrand in equation (\ref  {1.3}) shown by the red line with the integral being the blue shaded area under graph. It also shows what two values in parameter space would give the frequency $3$ and $1$.\relax }}{7}{figure.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig1.1}{{1.1}{7}{Plot of the integrand in equation (\ref {1.3}) shown by the red line with the integral being the blue shaded area under graph. It also shows what two values in parameter space would give the frequency $3$ and $1$.\relax }{figure.caption.2}{}}
\citation{3}
\newlabel{1.4}{{1.4}{8}{Tricky Denominators}{equation.1.3.4}{}}
\newlabel{1.5}{{1.5}{8}{Tricky Denominators}{equation.1.3.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Markov Chains}{8}{section.1.4}}
\newlabel{markov chains}{{1.4}{8}{Markov Chains}{section.1.4}{}}
\newlabel{Markov Chain Def}{{1.6}{9}{Markov Chains}{equation.1.4.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.1}What's for dinner?}{9}{subsection.1.4.1}}
\newlabel{What's for Dinner}{{1.4.1}{9}{What's for dinner?}{subsection.1.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Graph for the transition of state with their corresponding probability.\relax }}{9}{figure.caption.3}}
\newlabel{figMarkovProcess}{{1.2}{9}{Graph for the transition of state with their corresponding probability.\relax }{figure.caption.3}{}}
\newlabel{Stationary Vector}{{1.7}{10}{What's for dinner?}{equation.1.4.7}{}}
\citation{5}
\newlabel{reversibility}{{1.8}{11}{What's for dinner?}{equation.1.4.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Coding}{11}{section.1.5}}
\newlabel{coding}{{1.5}{11}{Coding}{section.1.5}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Metropolis Algorithms}{12}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Metropolis}{{2}{12}{Metropolis Algorithms}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Random Walk Metropolis}{12}{section.2.1}}
\newlabel{Random Walk Metropolis}{{2.1}{12}{Random Walk Metropolis}{section.2.1}{}}
\newlabel{Metropolis Def}{{2.1.1}{12}{}{theorem.2.1.1}{}}
\newlabel{2.1}{{2.1}{13}{Random Walk Metropolis}{equation.2.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}A Stroll Through a Strange Park}{13}{subsection.2.1.1}}
\newlabel{Metropolis Example}{{2.1.1}{13}{A Stroll Through a Strange Park}{subsection.2.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Plot of the exact posterior we want the sampler to estimate. We will use this to comment on the accuracy of the Random Walk Metropolis algorithm.\relax }}{14}{figure.caption.4}}
\newlabel{fig1.2}{{2.1}{14}{Plot of the exact posterior we want the sampler to estimate. We will use this to comment on the accuracy of the Random Walk Metropolis algorithm.\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Plot of how the parameter $x$ randomly ``walks'' through parameter space in each iteration of the algorithm.\relax }}{14}{figure.caption.5}}
\newlabel{fig1.3}{{2.2}{14}{Plot of how the parameter $x$ randomly ``walks'' through parameter space in each iteration of the algorithm.\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Plot of how each point of the posterior estimate is connected to the previous point.\relax }}{15}{figure.caption.6}}
\newlabel{fig1.4}{{2.3}{15}{Plot of how each point of the posterior estimate is connected to the previous point.\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Plot of the posterior approximation after 200 iterations without connections.\relax }}{15}{figure.caption.7}}
\newlabel{fig1.5}{{2.4}{15}{Plot of the posterior approximation after 200 iterations without connections.\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Metropolis Hastings}{16}{section.2.2}}
\newlabel{Metropolis Hastings}{{2.2}{16}{Metropolis Hastings}{section.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Plot of a symmetric proposal distribution, shown using a red line, with a shaded region that shows all of the potential samples we would lose due to a boundary constraint at zero. The blue dashed line is the mean of the proposal distribution; which would be the current parameter value in the sampling algorithm.\relax }}{16}{figure.caption.8}}
\newlabel{fig1.7}{{2.5}{16}{Plot of a symmetric proposal distribution, shown using a red line, with a shaded region that shows all of the potential samples we would lose due to a boundary constraint at zero. The blue dashed line is the mean of the proposal distribution; which would be the current parameter value in the sampling algorithm.\relax }{figure.caption.8}{}}
\newlabel{MH def}{{2.2.1}{17}{}{theorem.2.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}The Lifespan of a Monster}{17}{subsection.2.2.1}}
\newlabel{Metropolis-Hastings Example}{{2.2.1}{17}{The Lifespan of a Monster}{subsection.2.2.1}{}}
\newlabel{2.2}{{2.2}{17}{The Lifespan of a Monster}{equation.2.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Plot of the recapture data using a red line and the Poisson model with mean $\lambda _t = 1000\times \qopname  \relax o{exp}(-0.1\times t)\times 0.05$ using a blue line.\relax }}{18}{figure.caption.9}}
\newlabel{fig1.6}{{2.6}{18}{Plot of the recapture data using a red line and the Poisson model with mean $\lambda _t = 1000\times \exp (-0.1\times t)\times 0.05$ using a blue line.\relax }{figure.caption.9}{}}
\newlabel{fig1.8:a}{{\caption@xref {fig1.8:a}{ on input line 613}}{19}{The Lifespan of a Monster}{figure.caption.10}{}}
\newlabel{sub@fig1.8:a}{{}{19}{The Lifespan of a Monster}{figure.caption.10}{}}
\newlabel{fig1.8:b}{{\caption@xref {fig1.8:b}{ on input line 619}}{19}{The Lifespan of a Monster}{figure.caption.10}{}}
\newlabel{sub@fig1.8:b}{{}{19}{The Lifespan of a Monster}{figure.caption.10}{}}
\newlabel{fig1.8:c}{{\caption@xref {fig1.8:c}{ on input line 624}}{19}{The Lifespan of a Monster}{figure.caption.10}{}}
\newlabel{sub@fig1.8:c}{{}{19}{The Lifespan of a Monster}{figure.caption.10}{}}
\newlabel{fig1.8:d}{{\caption@xref {fig1.8:d}{ on input line 629}}{19}{The Lifespan of a Monster}{figure.caption.10}{}}
\newlabel{sub@fig1.8:d}{{}{19}{The Lifespan of a Monster}{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces The two graphs at the top show the values that parameters $\mu $ and $\psi $ take with each iteration. The ``flat'' sections are for when many potential samples were rejected and the algorithm returned to the previous sample. The bottom graphs show the histogram along with the probability density function and the mean value for parameters $\mu $ and $\psi $. PDFs are plotted using solid lines and means are plotted using dashed lines.\relax }}{19}{figure.caption.10}}
\newlabel{fig1.8}{{2.7}{19}{The two graphs at the top show the values that parameters $\mu $ and $\psi $ take with each iteration. The ``flat'' sections are for when many potential samples were rejected and the algorithm returned to the previous sample. The bottom graphs show the histogram along with the probability density function and the mean value for parameters $\mu $ and $\psi $. PDFs are plotted using solid lines and means are plotted using dashed lines.\relax }{figure.caption.10}{}}
\citation{3}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Plot of the confidence ellipse for 68.2\% which is equal to 1 standard deviation in blue and 95.8\% which is equal to 2 standard deviations in red.\relax }}{20}{figure.caption.11}}
\newlabel{fig1.9}{{2.8}{20}{Plot of the confidence ellipse for 68.2\% which is equal to 1 standard deviation in blue and 95.8\% which is equal to 2 standard deviations in red.\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Final plot of the model for estimating mosquito lifetime using the accepted values shown in Figure \ref  {fig1.9}. The best fit line is plotted with the mean values of $\mu $ and $\psi $ using a blue line. The shaded region shows the confidence intervals. The darker region is a 68.2\% confidence interval ($1\sigma _{\mu ,\psi }$) and the lighter region is a 95.8\% confidence interval ($2\sigma _{\mu ,\psi }$)\relax }}{20}{figure.caption.12}}
\newlabel{fig1.10}{{2.9}{20}{Final plot of the model for estimating mosquito lifetime using the accepted values shown in Figure \ref {fig1.9}. The best fit line is plotted with the mean values of $\mu $ and $\psi $ using a blue line. The shaded region shows the confidence intervals. The darker region is a 68.2\% confidence interval ($1\sigma _{\mu ,\psi }$) and the lighter region is a 95.8\% confidence interval ($2\sigma _{\mu ,\psi }$)\relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Gibbs \& Bayesian Networks}{21}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Gibbs}{{3}{21}{Gibbs \& Bayesian Networks}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}The Gibbs Sampler}{21}{section.3.1}}
\newlabel{The Gibbs Sampler}{{3.1}{21}{The Gibbs Sampler}{section.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces This graph shows the path between all of the accepted parameter values for the Metropolis-Hastings algorithm. Notice how, after 4000 iteration, only 46 values were accepted. Though here we have converged to a good estimate in this run, it shows how the high rate of rejection can impede the algorithms ability to converge. Many more iterations are required to increase the reliability of the algorithm.\relax }}{22}{figure.caption.13}}
\newlabel{fig3.1}{{3.1}{22}{This graph shows the path between all of the accepted parameter values for the Metropolis-Hastings algorithm. Notice how, after 4000 iteration, only 46 values were accepted. Though here we have converged to a good estimate in this run, it shows how the high rate of rejection can impede the algorithms ability to converge. Many more iterations are required to increase the reliability of the algorithm.\relax }{figure.caption.13}{}}
\newlabel{gibbs def}{{3.1.1}{22}{}{theorem.3.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Crime \& Unemployment}{23}{subsection.3.1.1}}
\newlabel{Crime Example}{{3.1.1}{23}{Crime \& Unemployment}{subsection.3.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces This graph shows the path for all parameter values of the Gibbs algorithm. Here there is no reject or accept condition like the Metropolis algorithm. All the parameter values are accepted allowing for faster convergence. This was produced using 500 iterations, far less than the Metropolis algorithms would need to converge.\relax }}{23}{figure.caption.14}}
\newlabel{fig3.5}{{3.2}{23}{This graph shows the path for all parameter values of the Gibbs algorithm. Here there is no reject or accept condition like the Metropolis algorithm. All the parameter values are accepted allowing for faster convergence. This was produced using 500 iterations, far less than the Metropolis algorithms would need to converge.\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces This graph shows how the Gibbs algorithm struggles to explore parameter space when there is a strong correlation between the parameter.\relax }}{24}{figure.caption.15}}
\newlabel{fig3.6}{{3.3}{24}{This graph shows how the Gibbs algorithm struggles to explore parameter space when there is a strong correlation between the parameter.\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces A Bayesian network for why the grass is wet.\relax }}{24}{figure.caption.16}}
\newlabel{fig2.1}{{3.4}{24}{A Bayesian network for why the grass is wet.\relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Why Is the Grass Wet?}{25}{subsection.3.1.2}}
\newlabel{Gibbs Example}{{3.1.2}{25}{Why Is the Grass Wet?}{subsection.3.1.2}{}}
\newlabel{code1}{{3.1}{25}{This function is working like a truth table where we have two variable Cloudy and Rained, and the 4 possible truth states. I used the slightly shorter notation, i.e. $P(C\mid R)$ rather than $P(C\mid R,S,W)$, because in this problem we know that the sprinklers went on and that the grass is wet. So for all four states listed above, $S$ and $W$ are always true so we can drop them out as long as we remember why}{lstlisting.3.1}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {3.1}This function is working like a truth table where we have two variable Cloudy and Rained, and the 4 possible truth states. I used the slightly shorter notation, i.e. $P(C\mid R)$ rather than $P(C\mid R,S,W)$, because in this problem we know that the sprinklers went on and that the grass is wet. So for all four states listed above, $S$ and $W$ are always true so we can drop them out as long as we remember why.}{25}{lstlisting.3.1}}
\newlabel{code2}{{3.2}{26}{This is how we will decide which state to update, either the state Cloudy or the state Rained}{lstlisting.3.2}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {3.2}This is how we will decide which state to update, either the state Cloudy or the state Rained.}{26}{lstlisting.3.2}}
\newlabel{code3}{{3.3}{26}{This is how we will decide what to update the chosen state to, where the probability of success is dependent on the value of the variable $\text {Probability}$}{lstlisting.3.3}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {3.3}This is how we will decide what to update the chosen state to, where the probability of success is dependent on the value of the variable $\text  {Probability}$.}{26}{lstlisting.3.3}}
\newlabel{fig3.3:a}{{\caption@xref {fig3.3:a}{ on input line 873}}{27}{Why Is the Grass Wet?}{figure.caption.17}{}}
\newlabel{sub@fig3.3:a}{{}{27}{Why Is the Grass Wet?}{figure.caption.17}{}}
\newlabel{fig3.3:b}{{\caption@xref {fig3.3:b}{ on input line 879}}{27}{Why Is the Grass Wet?}{figure.caption.17}{}}
\newlabel{sub@fig3.3:b}{{}{27}{Why Is the Grass Wet?}{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces These two graphs show the histogram along with thePDF and the mean value for parameters $C$ and $R$. The PDFs are plotted using solid lines and the means are plotted using dashed lines. Since they only take values True and False we must convert these to numerical values $1$ and $0$. This is the reason for there being only two levels of frequency.\relax }}{27}{figure.caption.17}}
\newlabel{fig3.3}{{3.5}{27}{These two graphs show the histogram along with thePDF and the mean value for parameters $C$ and $R$. The PDFs are plotted using solid lines and the means are plotted using dashed lines. Since they only take values True and False we must convert these to numerical values $1$ and $0$. This is the reason for there being only two levels of frequency.\relax }{figure.caption.17}{}}
\citation{3}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Plot of how the Gibbs algorithm moves through parameter space for both parameters. If I were to plot all the accepted values then we just have a unit square so I have restricted the data to the first 15 values. Within these 15 values the sampler moves from (1,1) to (0,1), back to (1,1), then to (1,0), and finally to (0,0).\relax }}{28}{figure.caption.18}}
\newlabel{fig3.4}{{3.6}{28}{Plot of how the Gibbs algorithm moves through parameter space for both parameters. If I were to plot all the accepted values then we just have a unit square so I have restricted the data to the first 15 values. Within these 15 values the sampler moves from (1,1) to (0,1), back to (1,1), then to (1,0), and finally to (0,0).\relax }{figure.caption.18}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Applications of Machine Learning}{29}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Applications of Machine Learning}{{4}{29}{Applications of Machine Learning}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Na\"{i}ve Bayesian Algorithm}{29}{section.4.1}}
\newlabel{Naive Bayesian Algorithm}{{4.1}{29}{Na\"{i}ve Bayesian Algorithm}{section.4.1}{}}
\newlabel{naive def}{{4.1.1}{29}{}{theorem.4.1.1}{}}
\newlabel{4.1}{{4.1}{29}{}{equation.4.1.1}{}}
\newlabel{4.2}{{4.2}{30}{}{equation.4.1.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Binary Classification}{30}{section.4.2}}
\newlabel{Binary Classification}{{4.2}{30}{Binary Classification}{section.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}To Play or not to Play}{31}{subsection.4.2.1}}
\newlabel{play example}{{4.2.1}{31}{To Play or not to Play}{subsection.4.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Is It Too Hot, or Too Cloudy, or Both to Play Ball?}{32}{subsection.4.2.2}}
\newlabel{temp example}{{4.2.2}{32}{Is It Too Hot, or Too Cloudy, or Both to Play Ball?}{subsection.4.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Scikit-Learn}{32}{subsection.4.2.3}}
\newlabel{Scikit-learn}{{4.2.3}{32}{Scikit-Learn}{subsection.4.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.4}Politics or Sports}{34}{subsection.4.2.4}}
\newlabel{sports example}{{4.2.4}{34}{Politics or Sports}{subsection.4.2.4}{}}
\citation{4}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Multiple Classification}{36}{section.4.3}}
\newlabel{Multiple Classification}{{4.3}{36}{Multiple Classification}{section.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Is This Wine Locally Sourced}{37}{subsection.4.3.1}}
\newlabel{wine example}{{4.3.1}{37}{Is This Wine Locally Sourced}{subsection.4.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Plot of the average accuracy for the Gaussian na\"{i}ve Bayes classifier for the percentage of dataset used for training. It takes the form of the logarithmic progression. Before even using just 20\% of the data as training we already have an average accuracy greater than 80\%.\relax }}{38}{figure.caption.20}}
\newlabel{fig4.2}{{4.1}{38}{Plot of the average accuracy for the Gaussian na\"{i}ve Bayes classifier for the percentage of dataset used for training. It takes the form of the logarithmic progression. Before even using just 20\% of the data as training we already have an average accuracy greater than 80\%.\relax }{figure.caption.20}{}}
\bibstyle{unsrt}
\bibdata{Ref}
\bibcite{1}{1}
\bibcite{2}{2}
\bibcite{3}{3}
\bibcite{5}{4}
\bibcite{4}{5}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Tables}{42}{appendix.A}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lot}{\contentsline {table}{\numberline {A.1}{\ignorespaces Data table for the weather and the play outcome, the frequency table for the weather, the likelihood table for the overall probabilities for weather and play, and the posterior probabilities table for Play. Here ``PP'' stands for Posterior Probability.\relax }}{42}{table.caption.24}}
\newlabel{tab:play}{{A.1}{42}{Data table for the weather and the play outcome, the frequency table for the weather, the likelihood table for the overall probabilities for weather and play, and the posterior probabilities table for Play. Here ``PP'' stands for Posterior Probability.\relax }{table.caption.24}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.2}{\ignorespaces Data table for the weather, temp and play, frequency table for the weather and the temp, the likelihood table for the overall probabilities for weather and play, and temp and play, and the posterior probabilities tables for play for weather and temp.\relax }}{43}{table.caption.27}}
\newlabel{tab:temp}{{A.2}{43}{Data table for the weather, temp and play, frequency table for the weather and the temp, the likelihood table for the overall probabilities for weather and play, and temp and play, and the posterior probabilities tables for play for weather and temp.\relax }{table.caption.27}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.3}{\ignorespaces Data table for the text and tag, and splitting text and adding test text, and the frequency tables for the text, text in Sports, and text in Politics before adding test text.\relax }}{44}{table.caption.32}}
\newlabel{tab:text}{{A.3}{44}{Data table for the text and tag, and splitting text and adding test text, and the frequency tables for the text, text in Sports, and text in Politics before adding test text.\relax }{table.caption.32}{}}
